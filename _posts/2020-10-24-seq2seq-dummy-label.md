---
title: Seq2Seq模型中的标签
tags: NLP
---

seq2seq模型一般由一个编码器和一个解码器组成。编码器将token的embedding序列编码成分布式表示（可能是向量序列或者一两个单独的语义向量），然后解码器根据编码器的结果生成出一个向量的序列。解码器生成的向量序列根据下游任务的不同有不同的利用方式。

**在使用seq2seq模型时，由于一些原因我们需要添加一些特殊的标签，或者叫dummy token，用来表示序列开始、序列结束、句子/段落的开始结束等等。**下面举例说明。

假设某seq2seq模型被用作英中翻译，且我们的样本是这样的：

```
x: "I love you."
y: "我爱你。"
```

那么**编码器的输入**应当是下列序列的embedding：

```
<SOS> i love you . <EOS>
```

这里`<SOS>`代表序列开始（Start Of Sequence），`<EOS>`代表序列结束（End Of Sequence），用什么符号表示并不重要，只要你喜欢并且它不在原始文本中就可以了。对于输入序列，开始标签可有可无，比较重要的标签是`<EOS>`。因为我们希望编码器通过看到结束标签来意识到输入序列已经结束，使得它输出整个输入的分布式表示。如果没有结束标签，可能编码器输出的向量更多地包含最后一个词附近局部的信息，而不是全局的信息。

**解码器的输入**应当是下列序列的embedding（假设我们用teacher forcing的方式）：

```
<SOS> 我 爱 你 。
```

> 为什么解码器也需要输入？
> 解码器一般是这样工作的：输入上一个词，输出该词最有可能的下一个词。
> 在训练时，”上一个词“一般使用标准答案，而不是真的使用解码器上一步预测出的词，这是因为如果让解码器”自由发挥“，模型会很难收敛。这种使用标准答案作为解码器输入的训练方式叫做teacher forcing。
> 在实际工作（预测）时，是没有标准答案的，所以只好使用解码器上一步预测出的词作为当前步的输入。
> 这种训练和预测时的不一致性，使得seq2seq模型有可能在训练时表现还算不错，但是预测时表现很差，这种现象被称为exposure bias。
> 有很多方法可以缓解exposure bias，一种方法就是在训练时并不是一直teacher forcing，偶尔也让解码器自由发挥看看。

这相当于把原始的标准答案往右平移了一个token，在Transformer论文里被称为 *shifted right* 的输出。

**我们希望得到的目标输出**，也就是解码器用来计算loss的输出，或者说样本的标签应当是这样的：

```
我 爱 你 。 <EOS>
```

这里有个结束标签，是因为我们期待解码器自己知道该在什么时候停止输出，并在此刻输出结束标签提醒我们。

> 注意：模型有可能一开始就输出`<EOS>`，或者一直不输出`<EOS>`，所以我们不能完全按照结束标签来截断输出，而是应当仅考虑在某个长度范围内出现的`<EOS>`：太早输出的`<EOS>`我们忽视它，如果到达我们设定好的某个最大长度还不输出`<EOS>`，也不会再让解码器再继续下去了。

另外，我们也有可能在输入序列中添加更多特殊的标签，来嵌入更多先验的有用信息，或者让编码器在读到特殊标签时输出有特殊用途的向量表示。或者在输出序列中添加更多的特殊标签，以期待解码器在某些特殊时刻输出这些特殊标签来实现更复杂的功能。
